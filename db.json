{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.6.4.min.js","path":"js/jquery-3.6.4.min.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/reading-DiT-code.md","hash":"382d1fe6bef9a3d7fb83802af99372648abc1c7d","modified":1770376739859},{"_id":"source/.DS_Store","hash":"e947f69ee02bfb0b1ff4b8a0984df6972949d12d","modified":1770059466408},{"_id":"source/_posts/init.md","hash":"50957e0dced31b84eb20247b93d010a43d3479cd","modified":1770057436323},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1770053892268},{"_id":"node_modules/hexo-theme-landscape/scripts/fancybox.js","hash":"c857d7a5e4a5d71c743a009c5932bf84229db428","modified":1770053976117},{"_id":"node_modules/hexo-theme-landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1770053975881},{"_id":"node_modules/hexo-theme-landscape/package.json","hash":"06889bee30e4c39479467021da434d3a6a0990fc","modified":1770053976170},{"_id":"node_modules/hexo-theme-landscape/layout/archive.ejs","hash":"97160b8111dd0283f8231408bcab4c87d31c1646","modified":1770053975882},{"_id":"node_modules/hexo-theme-landscape/_config.yml","hash":"a93d7b3990e45bc7247eecf01888f71674887a63","modified":1770053976177},{"_id":"node_modules/hexo-theme-landscape/layout/category.ejs","hash":"97160b8111dd0283f8231408bcab4c87d31c1646","modified":1770053975883},{"_id":"node_modules/hexo-theme-landscape/layout/layout.ejs","hash":"0d1765036e4874500e68256fedb7470e96eeb6ee","modified":1770053975886},{"_id":"node_modules/hexo-theme-landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1770053975887},{"_id":"node_modules/hexo-theme-landscape/README.md","hash":"6497b70356271fd6f9f1dc862353be844c457a53","modified":1770053976171},{"_id":"node_modules/hexo-theme-landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1770053975887},{"_id":"node_modules/hexo-theme-landscape/layout/tag.ejs","hash":"97160b8111dd0283f8231408bcab4c87d31c1646","modified":1770053975890},{"_id":"node_modules/hexo-theme-landscape/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1770053976178},{"_id":"node_modules/hexo-theme-landscape/languages/en-US.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/de-DE.yml","hash":"d29d1c4256b7ed9df42f511c2ff0a23ad5fd6c1f","modified":1770053976178},{"_id":"node_modules/hexo-theme-landscape/layout/index.ejs","hash":"57281fc3812c877ec2d8e89ec87ede57b9789d4c","modified":1770053975886},{"_id":"node_modules/hexo-theme-landscape/languages/en.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/es-ES.yml","hash":"7008a8fc91f18d2a735864817b8ebda30c7a2c66","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/fr-FR.yml","hash":"8d09dbdab00a30a2870b56f7c0a7ca7deafa7b88","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/hu-HU.yml","hash":"712d18664898fa21ba38d4973e90ef41a324ea25","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/hu.yml","hash":"284d557130bf54a74e7dcef9d42096130e4d9550","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/it-IT.yml","hash":"2cb6dc2fab9bd2dbe1c8bb869a9e8bf85a564fdd","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/it.yml","hash":"89b7d91306b2c1a0f3ac023b657bf974f798a1e8","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/ja-JP.yml","hash":"08481267e0c112e1f6855620f2837ec4c4a98bbd","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/languages/ko-KR.yml","hash":"19209ad8f9d4057e8df808937f950eb265e1db69","modified":1770053976180},{"_id":"node_modules/hexo-theme-landscape/languages/mn.yml","hash":"2e7523951072a9403ead3840ad823edd1084c116","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/languages/en-GB.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/languages/pt-PT.yml","hash":"0f852b6b228e6ea59aa3540574bb89b233f2a098","modified":1770053976182},{"_id":"node_modules/hexo-theme-landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1770053976182},{"_id":"node_modules/hexo-theme-landscape/languages/nl-NL.yml","hash":"5ebbc30021f05d99938f96dfff280392df7f91f0","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/languages/ru-RU.yml","hash":"360d11a28bb768afb1dd15f63fa7fd3a8cc547ee","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/languages/th.yml","hash":"84a55b00aa01f03982be294e43c33a20e6d32862","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/languages/tr.yml","hash":"a1cdbfa17682d7a971de8ab8588bf57c74224b5b","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/languages/zh-CN.yml","hash":"1efd95774f401c80193eac6ee3f1794bfe93dc5a","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1770053975882},{"_id":"node_modules/hexo-theme-landscape/languages/th-TH.yml","hash":"ebfdba9bc4842c829473c1e6e4544344f182724d","modified":1770053976183},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/after-footer.ejs","hash":"1b89d0caba03a66a43d9c290a5e94fa438a89210","modified":1770053975881},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/head.ejs","hash":"0e94f5722d4c44d3cc91be2f4fd30b9ab503b868","modified":1770053975885},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/article.ejs","hash":"e9d4678e14be5e3cd5e34d783e5af6d6626092f5","modified":1770053975882},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive.ejs","hash":"0039146b8ccbdf9b9f8bee58fc6c238f0e9921fc","modified":1770053975882},{"_id":"node_modules/hexo-theme-landscape/languages/mn-MN.yml","hash":"b9e5f3e7c0c2f779cf2cfded6db847b5941637ca","modified":1770053976181},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/footer.ejs","hash":"3656eb692254346671abc03cb3ba1459829e0dce","modified":1770053975884},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/google-analytics.ejs","hash":"2ea7442ea1e1a8ab4e41e26c563f58413b59a3d0","modified":1770053975884},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/gauges-analytics.ejs","hash":"21a1e2a3907d1a3dad1cd0ab855fe6735f233c74","modified":1770053975884},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/header.ejs","hash":"6a5033d189554c9a6d42e2ef7952ae5c9742648e","modified":1770053975886},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1770053975887},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1770053975889},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1770053975882},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1770053975883},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/recent_posts.ejs","hash":"60c4b012dcc656438ff59997e60367e5a21ab746","modified":1770053975889},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1770053975881},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1770053975890},{"_id":"node_modules/hexo-theme-landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1770053976179},{"_id":"node_modules/hexo-theme-landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1770053976171},{"_id":"node_modules/hexo-theme-landscape/source/css/_variables.styl","hash":"ca28281423ae57d76b6c1eb91cd845fd4e518bd6","modified":1770053976172},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","hash":"e55a1d92954ed20f6887f92dc727bb995a010a43","modified":1770053976177},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","hash":"49773efcb2221bbdf2d86f3f5c5ff2d841b528cc","modified":1770053976163},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1770053976172},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1770053976174},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/article.styl","hash":"f608400a08cf137ab15ec1f44bac551950afe879","modified":1770053976173},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/header.styl","hash":"268d2989acb06e2ddd06cc36a6918c6cd865476b","modified":1770053976174},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1770053976173},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1770053976176},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/highlight.styl","hash":"9cc3b2927d814f2f6e8e188f9d3657b94f4c6ef3","modified":1770053976176},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1770053975890},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1770053976177},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1770053976174},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1770053976177},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1770053976177},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1770053976176},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1770053975884},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/title.ejs","hash":"4d7e62574ddf46de9b41605fe3140d77b5ddb26d","modified":1770053975890},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1770053975889},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1770053975887},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1770053975883},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/date.ejs","hash":"f1458584b679545830b75bef2526e2f3eb931045","modified":1770053975883},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1770053976158},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.6.4.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1770053976143},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1770053976112},{"_id":"public/2026/02/06/reading-DiT-code/index.html","hash":"21a571d2d0d55f0bb441a2eaf8e54c4c563f9883","modified":1770376744775},{"_id":"public/2026/02/03/init/index.html","hash":"992623fde4fd359707973d9b01b7a8205adec711","modified":1770376744775},{"_id":"public/2026/02/03/hello-world/index.html","hash":"4af2befbe73da62e622eb3cbe5d9266770ba9fab","modified":1770376744775},{"_id":"public/index.html","hash":"30eaa30202cc73393d7d493d6a953b838c7007b8","modified":1770376744775},{"_id":"public/archives/index.html","hash":"67004b0d80a770f0b6a91536546eb6c1124c9b05","modified":1770376744775},{"_id":"public/archives/2026/index.html","hash":"0d2bd8835fa0e1947507d5b9a17cceb21652585c","modified":1770376744775},{"_id":"public/archives/2026/02/index.html","hash":"7953957ac570521262e6fde9158726f18bb5d79a","modified":1770376744775},{"_id":"public/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1770376744775},{"_id":"public/js/script.js","hash":"49773efcb2221bbdf2d86f3f5c5ff2d841b528cc","modified":1770376744775},{"_id":"public/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1770376744775},{"_id":"public/css/style.css","hash":"ecc329be740a220cc188ff49b02da4847cb7ee5e","modified":1770376744775},{"_id":"public/js/jquery-3.6.4.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1770376744775},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1770376744775}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2026-02-02T17:12:34.320Z","updated":"2026-02-02T17:38:12.268Z","comments":1,"layout":"post","photos":[],"_id":"cmlaslr510000his67rhdficj","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"init","date":"2026-02-02T18:37:16.000Z","_content":"","source":"_posts/init.md","raw":"---\ntitle: init\ndate: 2026-02-03 02:37:16\ntags:\n---\n","slug":"init","published":1,"updated":"2026-02-02T18:37:16.323Z","comments":1,"layout":"post","photos":[],"_id":"cmlaslr530001his60ifkf00f","content":"","excerpt":"","more":""},{"title":"reading DiT code","date":"2026-02-05T19:28:36.000Z","_content":"\n# Time Embedding\n\nwe try to enrich the information of a time scalar!\n\nif we dont do so, the info that the model can get from the time scalar is poor.\n\n```python\ndef forward(self, t):\n        # t_freq [batch, ..., t, dim]\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        # t_emb [batch, ..., t, hidden]\n        return t_emb\n```\n\n这里的 `t` 其实就是 `[batch, ]`\n\nlike `[4,4,4,4,4,4,...]` \n\n`timestep_embedding` using sin and cos to encode the number\n\ncode is like:\n\n```python\n@staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        # freqs = exp(-log(max_period) * [0, ..., half - 1] / half)\n        # [dim // 2]\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=t.device)\n        # t 应该是位置信息 [batch, ..., t, 1]\n        # freqs [1, dim // 2]\n        # args -> [batch, ..., t, dim // 2]\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        # embedding -> [batch, ..., t, dim]\n        return embedding\n```\n\nafter the `timestep_embedding` we got the torch like [batch, dim]\n\nafter the mlp, we got [batch, hidden]\n\nthat is, TimestepEmbedder change from  `[batch]` to.`[batch, hidden]`\n\n\n\n# LabelEmbedder\n\nwe are using **Classifier-Free Guidance (CFG)** \n\n先考虑条件生成，模型有时候的生成结果和条件拟合的不够好。比如生成的图片结果和你的 `text prompt` 在语义上靠的还不够近。这时候，你希望生成的结果再靠近条件一点，和条件更加贴合！\n\n对于带条件的生成，我们往往需要先训练一个 Classifier，它的作用是对于给定的图像，输出它的分类类别。训练好了以后，他就有了看图的能力。\n\n在扩散的过程中，每次 sample 一个噪音，使用Classifier对 input 求梯度，用这个梯度去 modify sample出来的噪音，给这个noise以引导。然后用这个noise去更新 `x` \n\n使用CFG就不用一个额外的classfier了。训练的时候，他会随机的丢弃标签，进行无标签的生成\n\n形式化来讲，每次的输入是一个\n$$\n<x_t,t,c>\n$$\n**训练时：**\n\n我们有概率地（`p=0.1~0.2`）令\n$$\nc = \\empty\n$$\n 这样模型同时学会了无条件生成和带条件生成\n\n**推理时**\n\n走两次\n\n1. $$\n   input = <x_t,t, c>\n   $$\n\n2. $$\n   input=<x_t,t,\\empty>\n   $$\n\n然后两个结果进行综合\n$$\n\\hat{\\epsilon}=s\\epsilon_{conditioned}+(1-s)\\epsilon_{unconditioned}\n$$\n一般\n$$\ns = [5, 7]\n$$\n相当于有了一个 $\\epsilon_{uncond}$ 又有了一个 $\\epsilon_{cond}$  两者做差你就知道 $cond$ 的方向在哪里了！\n\n`LabelEmbedder` 的 `__init__`\n\n```python\ndef __init__(self, num_classes, hidden_size, dropout_prob):\n```\n\n这里 `dropout_prob` 应该是随机丢掉分类\n","source":"_posts/reading-DiT-code.md","raw":"---\ntitle: reading DiT code\ndate: 2026-02-06 03:28:36\ntags:\n---\n\n# Time Embedding\n\nwe try to enrich the information of a time scalar!\n\nif we dont do so, the info that the model can get from the time scalar is poor.\n\n```python\ndef forward(self, t):\n        # t_freq [batch, ..., t, dim]\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        # t_emb [batch, ..., t, hidden]\n        return t_emb\n```\n\n这里的 `t` 其实就是 `[batch, ]`\n\nlike `[4,4,4,4,4,4,...]` \n\n`timestep_embedding` using sin and cos to encode the number\n\ncode is like:\n\n```python\n@staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        # freqs = exp(-log(max_period) * [0, ..., half - 1] / half)\n        # [dim // 2]\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=t.device)\n        # t 应该是位置信息 [batch, ..., t, 1]\n        # freqs [1, dim // 2]\n        # args -> [batch, ..., t, dim // 2]\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        # embedding -> [batch, ..., t, dim]\n        return embedding\n```\n\nafter the `timestep_embedding` we got the torch like [batch, dim]\n\nafter the mlp, we got [batch, hidden]\n\nthat is, TimestepEmbedder change from  `[batch]` to.`[batch, hidden]`\n\n\n\n# LabelEmbedder\n\nwe are using **Classifier-Free Guidance (CFG)** \n\n先考虑条件生成，模型有时候的生成结果和条件拟合的不够好。比如生成的图片结果和你的 `text prompt` 在语义上靠的还不够近。这时候，你希望生成的结果再靠近条件一点，和条件更加贴合！\n\n对于带条件的生成，我们往往需要先训练一个 Classifier，它的作用是对于给定的图像，输出它的分类类别。训练好了以后，他就有了看图的能力。\n\n在扩散的过程中，每次 sample 一个噪音，使用Classifier对 input 求梯度，用这个梯度去 modify sample出来的噪音，给这个noise以引导。然后用这个noise去更新 `x` \n\n使用CFG就不用一个额外的classfier了。训练的时候，他会随机的丢弃标签，进行无标签的生成\n\n形式化来讲，每次的输入是一个\n$$\n<x_t,t,c>\n$$\n**训练时：**\n\n我们有概率地（`p=0.1~0.2`）令\n$$\nc = \\empty\n$$\n 这样模型同时学会了无条件生成和带条件生成\n\n**推理时**\n\n走两次\n\n1. $$\n   input = <x_t,t, c>\n   $$\n\n2. $$\n   input=<x_t,t,\\empty>\n   $$\n\n然后两个结果进行综合\n$$\n\\hat{\\epsilon}=s\\epsilon_{conditioned}+(1-s)\\epsilon_{unconditioned}\n$$\n一般\n$$\ns = [5, 7]\n$$\n相当于有了一个 $\\epsilon_{uncond}$ 又有了一个 $\\epsilon_{cond}$  两者做差你就知道 $cond$ 的方向在哪里了！\n\n`LabelEmbedder` 的 `__init__`\n\n```python\ndef __init__(self, num_classes, hidden_size, dropout_prob):\n```\n\n这里 `dropout_prob` 应该是随机丢掉分类\n","slug":"reading-DiT-code","published":1,"updated":"2026-02-06T11:18:59.859Z","comments":1,"layout":"post","photos":[],"_id":"cmlaslr540002his621i1etn9","content":"<h1 id=\"Time-Embedding\"><a href=\"#Time-Embedding\" class=\"headerlink\" title=\"Time Embedding\"></a>Time Embedding</h1><p>we try to enrich the information of a time scalar!</p>\n<p>if we dont do so, the info that the model can get from the time scalar is poor.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, t</span>):</span><br><span class=\"line\">        <span class=\"comment\"># t_freq [batch, ..., t, dim]</span></span><br><span class=\"line\">        t_freq = <span class=\"variable language_\">self</span>.timestep_embedding(t, <span class=\"variable language_\">self</span>.frequency_embedding_size)</span><br><span class=\"line\">        t_emb = <span class=\"variable language_\">self</span>.mlp(t_freq)</span><br><span class=\"line\">        <span class=\"comment\"># t_emb [batch, ..., t, hidden]</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> t_emb</span><br></pre></td></tr></table></figure>\n\n<p>这里的 <code>t</code> 其实就是 <code>[batch, ]</code></p>\n<p>like <code>[4,4,4,4,4,4,...]</code> </p>\n<p><code>timestep_embedding</code> using sin and cos to encode the number</p>\n<p>code is like:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@staticmethod</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">timestep_embedding</span>(<span class=\"params\">t, dim, max_period=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Create sinusoidal timestep embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        :param t: a 1-D Tensor of N indices, one per batch element.</span></span><br><span class=\"line\"><span class=\"string\">                          These may be fractional.</span></span><br><span class=\"line\"><span class=\"string\">        :param dim: the dimension of the output.</span></span><br><span class=\"line\"><span class=\"string\">        :param max_period: controls the minimum frequency of the embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        :return: an (N, D) Tensor of positional embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py</span></span><br><span class=\"line\">        half = dim // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"comment\"># freqs = exp(-log(max_period) * [0, ..., half - 1] / half)</span></span><br><span class=\"line\">        <span class=\"comment\"># [dim // 2]</span></span><br><span class=\"line\">        freqs = torch.exp(</span><br><span class=\"line\">            -math.log(max_period) * torch.arange(start=<span class=\"number\">0</span>, end=half, dtype=torch.float32) / half</span><br><span class=\"line\">        ).to(device=t.device)</span><br><span class=\"line\">        <span class=\"comment\"># t 应该是位置信息 [batch, ..., t, 1]</span></span><br><span class=\"line\">        <span class=\"comment\"># freqs [1, dim // 2]</span></span><br><span class=\"line\">        <span class=\"comment\"># args -&gt; [batch, ..., t, dim // 2]</span></span><br><span class=\"line\">        args = t[:, <span class=\"literal\">None</span>].<span class=\"built_in\">float</span>() * freqs[<span class=\"literal\">None</span>]</span><br><span class=\"line\">        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dim % <span class=\"number\">2</span>:</span><br><span class=\"line\">            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :<span class=\"number\">1</span>])], dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># embedding -&gt; [batch, ..., t, dim]</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> embedding</span><br></pre></td></tr></table></figure>\n\n<p>after the <code>timestep_embedding</code> we got the torch like [batch, dim]</p>\n<p>after the mlp, we got [batch, hidden]</p>\n<p>that is, TimestepEmbedder change from  <code>[batch]</code> to.<code>[batch, hidden]</code></p>\n<h1 id=\"LabelEmbedder\"><a href=\"#LabelEmbedder\" class=\"headerlink\" title=\"LabelEmbedder\"></a>LabelEmbedder</h1><p>we are using <strong>Classifier-Free Guidance (CFG)</strong> </p>\n<p>先考虑条件生成，模型有时候的生成结果和条件拟合的不够好。比如生成的图片结果和你的 <code>text prompt</code> 在语义上靠的还不够近。这时候，你希望生成的结果再靠近条件一点，和条件更加贴合！</p>\n<p>对于带条件的生成，我们往往需要先训练一个 Classifier，它的作用是对于给定的图像，输出它的分类类别。训练好了以后，他就有了看图的能力。</p>\n<p>在扩散的过程中，每次 sample 一个噪音，使用Classifier对 input 求梯度，用这个梯度去 modify sample出来的噪音，给这个noise以引导。然后用这个noise去更新 <code>x</code> </p>\n<p>使用CFG就不用一个额外的classfier了。训练的时候，他会随机的丢弃标签，进行无标签的生成</p>\n<p>形式化来讲，每次的输入是一个<br>$$<br>&lt;x_t,t,c&gt;<br>$$<br><strong>训练时：</strong></p>\n<p>我们有概率地（<code>p=0.1~0.2</code>）令<br>$$<br>c &#x3D; \\empty<br>$$<br> 这样模型同时学会了无条件生成和带条件生成</p>\n<p><strong>推理时</strong></p>\n<p>走两次</p>\n<ol>\n<li><p>$$<br>input &#x3D; &lt;x_t,t, c&gt;<br>$$</p>\n</li>\n<li><p>$$<br>input&#x3D;&lt;x_t,t,\\empty&gt;<br>$$</p>\n</li>\n</ol>\n<p>然后两个结果进行综合<br>$$<br>\\hat{\\epsilon}&#x3D;s\\epsilon_{conditioned}+(1-s)\\epsilon_{unconditioned}<br>$$<br>一般<br>$$<br>s &#x3D; [5, 7]<br>$$<br>相当于有了一个 $\\epsilon_{uncond}$ 又有了一个 $\\epsilon_{cond}$  两者做差你就知道 $cond$ 的方向在哪里了！</p>\n<p><code>LabelEmbedder</code> 的 <code>__init__</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_classes, hidden_size, dropout_prob</span>):</span><br></pre></td></tr></table></figure>\n\n<p>这里 <code>dropout_prob</code> 应该是随机丢掉分类</p>\n","excerpt":"","more":"<h1 id=\"Time-Embedding\"><a href=\"#Time-Embedding\" class=\"headerlink\" title=\"Time Embedding\"></a>Time Embedding</h1><p>we try to enrich the information of a time scalar!</p>\n<p>if we dont do so, the info that the model can get from the time scalar is poor.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, t</span>):</span><br><span class=\"line\">        <span class=\"comment\"># t_freq [batch, ..., t, dim]</span></span><br><span class=\"line\">        t_freq = <span class=\"variable language_\">self</span>.timestep_embedding(t, <span class=\"variable language_\">self</span>.frequency_embedding_size)</span><br><span class=\"line\">        t_emb = <span class=\"variable language_\">self</span>.mlp(t_freq)</span><br><span class=\"line\">        <span class=\"comment\"># t_emb [batch, ..., t, hidden]</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> t_emb</span><br></pre></td></tr></table></figure>\n\n<p>这里的 <code>t</code> 其实就是 <code>[batch, ]</code></p>\n<p>like <code>[4,4,4,4,4,4,...]</code> </p>\n<p><code>timestep_embedding</code> using sin and cos to encode the number</p>\n<p>code is like:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@staticmethod</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">timestep_embedding</span>(<span class=\"params\">t, dim, max_period=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Create sinusoidal timestep embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        :param t: a 1-D Tensor of N indices, one per batch element.</span></span><br><span class=\"line\"><span class=\"string\">                          These may be fractional.</span></span><br><span class=\"line\"><span class=\"string\">        :param dim: the dimension of the output.</span></span><br><span class=\"line\"><span class=\"string\">        :param max_period: controls the minimum frequency of the embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        :return: an (N, D) Tensor of positional embeddings.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py</span></span><br><span class=\"line\">        half = dim // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"comment\"># freqs = exp(-log(max_period) * [0, ..., half - 1] / half)</span></span><br><span class=\"line\">        <span class=\"comment\"># [dim // 2]</span></span><br><span class=\"line\">        freqs = torch.exp(</span><br><span class=\"line\">            -math.log(max_period) * torch.arange(start=<span class=\"number\">0</span>, end=half, dtype=torch.float32) / half</span><br><span class=\"line\">        ).to(device=t.device)</span><br><span class=\"line\">        <span class=\"comment\"># t 应该是位置信息 [batch, ..., t, 1]</span></span><br><span class=\"line\">        <span class=\"comment\"># freqs [1, dim // 2]</span></span><br><span class=\"line\">        <span class=\"comment\"># args -&gt; [batch, ..., t, dim // 2]</span></span><br><span class=\"line\">        args = t[:, <span class=\"literal\">None</span>].<span class=\"built_in\">float</span>() * freqs[<span class=\"literal\">None</span>]</span><br><span class=\"line\">        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dim % <span class=\"number\">2</span>:</span><br><span class=\"line\">            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :<span class=\"number\">1</span>])], dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># embedding -&gt; [batch, ..., t, dim]</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> embedding</span><br></pre></td></tr></table></figure>\n\n<p>after the <code>timestep_embedding</code> we got the torch like [batch, dim]</p>\n<p>after the mlp, we got [batch, hidden]</p>\n<p>that is, TimestepEmbedder change from  <code>[batch]</code> to.<code>[batch, hidden]</code></p>\n<h1 id=\"LabelEmbedder\"><a href=\"#LabelEmbedder\" class=\"headerlink\" title=\"LabelEmbedder\"></a>LabelEmbedder</h1><p>we are using <strong>Classifier-Free Guidance (CFG)</strong> </p>\n<p>先考虑条件生成，模型有时候的生成结果和条件拟合的不够好。比如生成的图片结果和你的 <code>text prompt</code> 在语义上靠的还不够近。这时候，你希望生成的结果再靠近条件一点，和条件更加贴合！</p>\n<p>对于带条件的生成，我们往往需要先训练一个 Classifier，它的作用是对于给定的图像，输出它的分类类别。训练好了以后，他就有了看图的能力。</p>\n<p>在扩散的过程中，每次 sample 一个噪音，使用Classifier对 input 求梯度，用这个梯度去 modify sample出来的噪音，给这个noise以引导。然后用这个noise去更新 <code>x</code> </p>\n<p>使用CFG就不用一个额外的classfier了。训练的时候，他会随机的丢弃标签，进行无标签的生成</p>\n<p>形式化来讲，每次的输入是一个<br>$$<br>&lt;x_t,t,c&gt;<br>$$<br><strong>训练时：</strong></p>\n<p>我们有概率地（<code>p=0.1~0.2</code>）令<br>$$<br>c &#x3D; \\empty<br>$$<br> 这样模型同时学会了无条件生成和带条件生成</p>\n<p><strong>推理时</strong></p>\n<p>走两次</p>\n<ol>\n<li><p>$$<br>input &#x3D; &lt;x_t,t, c&gt;<br>$$</p>\n</li>\n<li><p>$$<br>input&#x3D;&lt;x_t,t,\\empty&gt;<br>$$</p>\n</li>\n</ol>\n<p>然后两个结果进行综合<br>$$<br>\\hat{\\epsilon}&#x3D;s\\epsilon_{conditioned}+(1-s)\\epsilon_{unconditioned}<br>$$<br>一般<br>$$<br>s &#x3D; [5, 7]<br>$$<br>相当于有了一个 $\\epsilon_{uncond}$ 又有了一个 $\\epsilon_{cond}$  两者做差你就知道 $cond$ 的方向在哪里了！</p>\n<p><code>LabelEmbedder</code> 的 <code>__init__</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_classes, hidden_size, dropout_prob</span>):</span><br></pre></td></tr></table></figure>\n\n<p>这里 <code>dropout_prob</code> 应该是随机丢掉分类</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}